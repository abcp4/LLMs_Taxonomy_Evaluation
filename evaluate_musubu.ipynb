{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fafb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxo_name = \"food\"\n",
    "# taxo = \"taxos/wn_food.taxo.txt\"\n",
    "taxo = f\"taxos/{taxo_name}.taxo\"\n",
    "# taxo = \"taxos/equipment.taxo.txt\"\n",
    "is_parent = {}\n",
    "for line in open(taxo):\n",
    "    #split by tab\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    is_parent[line[1]] = line[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dfa009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ancestors(taxo, term):\n",
    "    \"\"\"\n",
    "    Get all ancestors of a term in the taxonomy\n",
    "    \"\"\"\n",
    "    ancestors = []\n",
    "    while term in is_parent:\n",
    "        term = is_parent[term]\n",
    "        ancestors.append(term)\n",
    "    return ancestors\n",
    "\n",
    "def get_children(taxo, term):\n",
    "    \"\"\"\n",
    "    Get all children of a term in the taxonomy\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for line in open(taxo):\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        if line[2] == term:\n",
    "            children.append(line[1])\n",
    "    return children\n",
    "\n",
    "#get all tree except the term and its children\n",
    "#Format:\n",
    "# root child 1 child 2\n",
    "# child 1 child 3 child 4\n",
    "#iterative version\n",
    "def get_tree_it(taxo,root, term):\n",
    "    \"\"\"\n",
    "    Get all terms in the taxonomy except the term and its children\n",
    "    \"\"\"\n",
    "    tree = \"\"\n",
    "    stack = [(root, 0)]\n",
    "    while stack:\n",
    "        node, depth = stack.pop()\n",
    "        if depth > 4:\n",
    "            continue\n",
    "        tree += \"\\nParent: \" + node \n",
    "        root_children = get_children(taxo, node)\n",
    "        if len(root_children) != 0:\n",
    "            tree = tree + \"\\nChildren: \"\n",
    "        for child in root_children:\n",
    "            if child == term:\n",
    "                continue\n",
    "            tree += child + \", \"\n",
    "            stack.append((child, depth + 1))\n",
    "    return tree\n",
    "\n",
    "def get_leafs(taxo,root, term):\n",
    "    \"\"\"\n",
    "    Get all terms in the taxonomy except the term and its children\n",
    "    \"\"\"\n",
    "    parents_leaves_dict = {}\n",
    "    stack = [(root, 0)]\n",
    "    while stack:\n",
    "        node, depth = stack.pop()\n",
    "        if depth > 5:\n",
    "            continue\n",
    "        root_children = get_children(taxo, node)\n",
    "        if len(root_children) == 0:\n",
    "            parent = is_parent[node]\n",
    "            if parent not in parents_leaves_dict:\n",
    "                parents_leaves_dict[parent] = [node]\n",
    "            else:\n",
    "                parents_leaves_dict[parent].append(node)\n",
    "        # if term in root_children:\n",
    "        #     #get parent\n",
    "        #     parent = is_parent[child]\n",
    "        #     grand_parent = is_parent[parent]\n",
    "        #     if grand_parent not in parents_leaves_dict:\n",
    "        #         parents_leaves_dict[grand_parent] = [parent]\n",
    "        #     else:\n",
    "        #         parents_leaves_dict[grand_parent].append(parent)\n",
    "        #     continue\n",
    "        for child in root_children:\n",
    "            if child == term:\n",
    "                continue\n",
    "            stack.append((child, depth + 1))\n",
    "    return parents_leaves_dict\n",
    "\n",
    "def get_tree_leaves(parents_leaves_dict):\n",
    "    tree =\"\"\n",
    "    for parent in parents_leaves_dict:\n",
    "        if parent == \"food\":\n",
    "            continue\n",
    "        granparent = is_parent[parent]\n",
    "        tree += \"\\nGranparent: \" + granparent\n",
    "        tree += \"\\nParent: \" + parent\n",
    "        tree += \"\\nChildren: \"\n",
    "        for child in parents_leaves_dict[parent]:\n",
    "            tree += child + \", \"\n",
    "        tree += \"\\n\"\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6729c8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[   0    1    2    3    4    5    6    7    8    9   11   12   13   14\n",
      "   16   17   18   19   20   21   22   24   25   26   27   28   33   34\n",
      "   35   36   37   38   39   40   41   42   45   46   47   50   52   53\n",
      "   55   57   60   61   62   64   66   68   69   71   72   73   74   75\n",
      "   77   79   80   82   84   85   87   88   89   90   91   92   93   94\n",
      "   95   97   98  102  103  104  105  106  108  110  112  114  116  117\n",
      "  118  119  120  121  122  125  127  130  131  132  133  134  136  137\n",
      "  138  139  142  143  144  145  146  148  149  150  151  152  153  154\n",
      "  157  159  160  161  162  164  165  166  167  169  171  172  173  176\n",
      "  177  178  180  181  182  183  185  186  187  189  190  191  193  194\n",
      "  197  200  201  202  204  205  206  207  211  212  213  214  215  216\n",
      "  217  219  222  223  224  225  226  227  228  229  230  232  234  235\n",
      "  238  241  242  245  246  248  249  250  251  252  253  255  256  257\n",
      "  258  260  262  263  264  267  268  269  272  273  276  278  279  280\n",
      "  281  283  284  288  290  293  294  295  299  300  301  302  304  305\n",
      "  307  308  311  313  314  315  317  318  319  320  321  325  326  328\n",
      "  329  330  333  334  335  336  337  338  340  341  343  345  347  348\n",
      "  349  356  357  359  360  362  364  366  368  369  372  373  375  376\n",
      "  378  379  383  384  385  386  387  388  389  390  391  392  393  395\n",
      "  396  397  399  400  401  402  403  404  406  407  408  409  412  414\n",
      "  417  418  421  424  431  434  436  437  440  441  442  443  444  445\n",
      "  446  447  448  449  450  452  454  455  456  457  458  459  460  461\n",
      "  463  465  466  467  468  469  470  472  473  474  475  476  480  483\n",
      "  484  487  488  489  491  492  495  496  498  499  501  502  503  504\n",
      "  505  506  507  508  509  510  511  512  515  516  517  518  519  520\n",
      "  521  522  523  524  525  530  531  533  534  536  537  539  540  541\n",
      "  542  545  546  547  549  550  553  555  556  557  558  559  562  563\n",
      "  564  565  568  569  572  573  574  576  577  579  580  581  584  586\n",
      "  587  588  591  592  593  594  595  600  601  603  604  606  608  609\n",
      "  611  612  616  622  623  624  625  627  629  630  632  633  635  637\n",
      "  638  639  640  641  642  644  645  646  647  648  649  650  652  653\n",
      "  655  656  657  658  659  662  663  664  666  667  671  673  675  676\n",
      "  678  681  683  684  685  686  687  688  689  690  692  694  696  697\n",
      "  698  699  700  702  703  704  706  708  709  713  716  717  719  721\n",
      "  723  724  725  726  728  729  731  732  733  734  735  736  738  739\n",
      "  740  741  742  744  745  747  748  749  750  751  752  753  754  757\n",
      "  758  760  761  763  765  766  768  769  770  771  772  773  774  775\n",
      "  776  777  778  779  780  783  784  785  786  788  789  790  791  793\n",
      "  794  795  797  800  801  802  803  804  805  807  811  812  814  815\n",
      "  817  820  821  822  823  824  825  826  827  828  830  831  832  833\n",
      "  835  837  840  841  842  843  844  846  847  848  849  850  851  852\n",
      "  853  854  856  857  858  860  863  864  865  866  867  870  871  872\n",
      "  874  875  876  877  878  879  880  881  883  884  885  887  890  891\n",
      "  892  894  895  896  897  898  902  905  908  909  911  912  913  914\n",
      "  918  919  920  922  923  926  927  928  929  931  932  934  935  936\n",
      "  944  946  947  949  950  951  954  955  956  957  958  959  960  961\n",
      "  962  967  969  970  971  972  974  975  976  977  978  980  981  985\n",
      "  986  987  989  991  992  993  995  996  998  999 1000 1002 1003 1005\n",
      " 1007 1008 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021\n",
      " 1022 1023 1025 1026 1027 1028 1029 1031 1032 1035 1036 1038 1039 1043\n",
      " 1044 1045 1046 1049 1051 1052 1054 1056 1058 1059 1060 1062 1064 1065\n",
      " 1067 1069 1070 1071 1072 1074 1076 1077 1079 1081 1082 1083 1086 1088\n",
      " 1090 1092 1093 1094 1095 1096 1098 1100 1101 1103 1104 1107 1109 1110\n",
      " 1111 1112 1115 1116 1117 1118 1121 1122 1123 1124 1125 1126 1127 1128\n",
      " 1129 1130 1132 1133 1134 1135 1136 1137 1138 1139 1142 1143 1144 1146\n",
      " 1147 1148 1149 1150 1152 1153 1154 1155 1158 1160 1162 1163 1164 1167\n",
      " 1168 1169 1170 1171 1172 1178 1180 1181 1182 1183 1184 1186 1187 1188\n",
      " 1189 1190 1191 1192 1194 1196 1199 1200 1201 1204 1205 1207 1210 1211\n",
      " 1213 1214 1215 1217 1222 1224 1228 1232 1237 1238 1239 1241 1242 1243\n",
      " 1244 1245 1247 1248 1249 1250 1251 1253 1254 1256 1257 1258 1264 1265\n",
      " 1266 1267 1271 1273 1274 1275 1276 1277 1278 1281 1282 1284 1286 1287\n",
      " 1288 1290 1291 1292 1293 1294 1295 1296 1297 1299 1302 1303 1304 1306\n",
      " 1307 1308 1309 1310 1311 1314 1315 1317 1318 1319 1322 1324 1325 1326\n",
      " 1327 1329 1330 1331 1332 1336 1337 1338 1339 1340 1341 1343 1345 1346\n",
      " 1347 1348 1349 1350 1351 1352 1353 1354 1356 1358 1359 1360 1363 1365\n",
      " 1367 1368 1369 1370 1371 1373 1374 1375 1376 1377 1380 1381 1383 1385\n",
      " 1386 1387 1388 1390 1392 1393 1394 1395 1396 1397 1399 1400 1402 1404\n",
      " 1405 1406 1409 1410 1411 1413 1414 1415 1416 1417 1418 1420 1421 1422\n",
      " 1423 1424 1426 1427 1428 1431 1433 1436 1437 1441 1443 1444 1445 1447\n",
      " 1448 1449 1450 1451 1454 1456 1458 1459 1461 1462 1463 1464 1465 1466\n",
      " 1468 1469 1471 1472 1473 1476 1478 1482 1483 1484]\n",
      "  Test:  index=[  10   15   23   29   30   31   32   43   44   48   49   51   54   56\n",
      "   58   59   63   65   67   70   76   78   81   83   86   96   99  100\n",
      "  101  107  109  111  113  115  123  124  126  128  129  135  140  141\n",
      "  147  155  156  158  163  168  170  174  175  179  184  188  192  195\n",
      "  196  198  199  203  208  209  210  218  220  221  231  233  236  237\n",
      "  239  240  243  244  247  254  259  261  265  266  270  271  274  275\n",
      "  277  282  285  286  287  289  291  292  296  297  298  303  306  309\n",
      "  310  312  316  322  323  324  327  331  332  339  342  344  346  350\n",
      "  351  352  353  354  355  358  361  363  365  367  370  371  374  377\n",
      "  380  381  382  394  398  405  410  411  413  415  416  419  420  422\n",
      "  423  425  426  427  428  429  430  432  433  435  438  439  451  453\n",
      "  462  464  471  477  478  479  481  482  485  486  490  493  494  497\n",
      "  500  513  514  526  527  528  529  532  535  538  543  544  548  551\n",
      "  552  554  560  561  566  567  570  571  575  578  582  583  585  589\n",
      "  590  596  597  598  599  602  605  607  610  613  614  615  617  618\n",
      "  619  620  621  626  628  631  634  636  643  651  654  660  661  665\n",
      "  668  669  670  672  674  677  679  680  682  691  693  695  701  705\n",
      "  707  710  711  712  714  715  718  720  722  727  730  737  743  746\n",
      "  755  756  759  762  764  767  781  782  787  792  796  798  799  806\n",
      "  808  809  810  813  816  818  819  829  834  836  838  839  845  855\n",
      "  859  861  862  868  869  873  882  886  888  889  893  899  900  901\n",
      "  903  904  906  907  910  915  916  917  921  924  925  930  933  937\n",
      "  938  939  940  941  942  943  945  948  952  953  963  964  965  966\n",
      "  968  973  979  982  983  984  988  990  994  997 1001 1004 1006 1009\n",
      " 1024 1030 1033 1034 1037 1040 1041 1042 1047 1048 1050 1053 1055 1057\n",
      " 1061 1063 1066 1068 1073 1075 1078 1080 1084 1085 1087 1089 1091 1097\n",
      " 1099 1102 1105 1106 1108 1113 1114 1119 1120 1131 1140 1141 1145 1151\n",
      " 1156 1157 1159 1161 1165 1166 1173 1174 1175 1176 1177 1179 1185 1193\n",
      " 1195 1197 1198 1202 1203 1206 1208 1209 1212 1216 1218 1219 1220 1221\n",
      " 1223 1225 1226 1227 1229 1230 1231 1233 1234 1235 1236 1240 1246 1252\n",
      " 1255 1259 1260 1261 1262 1263 1268 1269 1270 1272 1279 1280 1283 1285\n",
      " 1289 1298 1300 1301 1305 1312 1313 1316 1320 1321 1323 1328 1333 1334\n",
      " 1335 1342 1344 1355 1357 1361 1362 1364 1366 1372 1378 1379 1382 1384\n",
      " 1389 1391 1398 1401 1403 1407 1408 1412 1419 1425 1429 1430 1432 1434\n",
      " 1435 1438 1439 1440 1442 1446 1452 1453 1455 1457 1460 1467 1470 1474\n",
      " 1475 1477 1479 1480 1481 1485]\n",
      "Fold 1:\n",
      "  Train: index=[   1    4    8   10   11   13   14   15   16   17   19   20   21   22\n",
      "   23   26   29   30   31   32   34   35   36   37   40   43   44   46\n",
      "   48   49   50   51   53   54   56   58   59   61   63   64   65   67\n",
      "   70   76   78   81   83   86   87   89   91   93   95   96   98   99\n",
      "  100  101  103  107  109  111  112  113  114  115  116  119  121  122\n",
      "  123  124  126  127  128  129  130  134  135  140  141  143  146  147\n",
      "  149  150  151  152  153  154  155  156  157  158  159  160  161  163\n",
      "  166  168  170  174  175  179  180  184  186  187  188  189  190  191\n",
      "  192  195  196  197  198  199  200  201  202  203  205  206  207  208\n",
      "  209  210  216  217  218  219  220  221  225  229  230  231  233  236\n",
      "  237  239  240  241  243  244  245  246  247  252  253  254  257  259\n",
      "  261  262  263  265  266  268  269  270  271  272  274  275  276  277\n",
      "  278  279  282  283  284  285  286  287  288  289  291  292  293  295\n",
      "  296  297  298  301  303  304  306  309  310  312  313  315  316  317\n",
      "  320  322  323  324  327  330  331  332  335  337  339  340  341  342\n",
      "  343  344  345  346  350  351  352  353  354  355  356  358  361  363\n",
      "  365  367  368  369  370  371  374  377  378  379  380  381  382  384\n",
      "  385  387  391  392  394  396  397  398  399  400  401  402  403  405\n",
      "  406  407  410  411  412  413  415  416  417  418  419  420  422  423\n",
      "  425  426  427  428  429  430  431  432  433  435  437  438  439  441\n",
      "  443  451  452  453  455  456  459  462  463  464  466  469  470  471\n",
      "  472  473  474  476  477  478  479  481  482  484  485  486  487  488\n",
      "  489  490  492  493  494  496  497  498  500  502  508  509  510  511\n",
      "  512  513  514  515  517  520  521  524  526  527  528  529  532  535\n",
      "  537  538  540  543  544  546  547  548  550  551  552  554  556  559\n",
      "  560  561  562  563  564  565  566  567  569  570  571  574  575  577\n",
      "  578  580  582  583  585  586  589  590  592  595  596  597  598  599\n",
      "  600  602  604  605  606  607  608  610  612  613  614  615  616  617\n",
      "  618  619  620  621  623  625  626  627  628  631  632  633  634  635\n",
      "  636  639  640  641  642  643  645  646  647  648  651  653  654  655\n",
      "  656  658  659  660  661  662  663  665  668  669  670  672  674  675\n",
      "  677  679  680  681  682  683  684  685  686  687  688  689  690  691\n",
      "  693  694  695  696  698  699  701  702  703  705  707  709  710  711\n",
      "  712  714  715  718  719  720  722  725  726  727  728  729  730  735\n",
      "  737  738  740  742  743  746  747  748  749  751  753  755  756  758\n",
      "  759  760  761  762  763  764  766  767  768  769  773  775  776  779\n",
      "  781  782  784  787  789  790  791  792  794  795  796  797  798  799\n",
      "  800  801  804  805  806  808  809  810  813  815  816  818  819  821\n",
      "  822  825  827  829  830  831  833  834  835  836  837  838  839  840\n",
      "  845  848  851  853  854  855  856  859  860  861  862  863  866  868\n",
      "  869  870  871  872  873  876  877  878  879  880  882  883  886  888\n",
      "  889  891  893  895  896  897  899  900  901  902  903  904  906  907\n",
      "  910  911  913  915  916  917  919  920  921  924  925  927  928  929\n",
      "  930  933  935  937  938  939  940  941  942  943  945  948  951  952\n",
      "  953  954  955  956  957  959  960  961  963  964  965  966  968  969\n",
      "  971  972  973  975  977  979  980  981  982  983  984  988  990  991\n",
      "  992  994  995  996  997  998 1001 1002 1003 1004 1006 1007 1008 1009\n",
      " 1011 1012 1014 1015 1016 1017 1019 1020 1021 1024 1025 1028 1029 1030\n",
      " 1033 1034 1037 1038 1039 1040 1041 1042 1044 1045 1046 1047 1048 1050\n",
      " 1051 1053 1055 1056 1057 1059 1060 1061 1062 1063 1064 1066 1068 1069\n",
      " 1070 1071 1072 1073 1075 1076 1077 1078 1079 1080 1081 1082 1084 1085\n",
      " 1086 1087 1089 1091 1092 1095 1097 1099 1100 1102 1104 1105 1106 1108\n",
      " 1109 1110 1113 1114 1115 1119 1120 1122 1123 1126 1127 1129 1130 1131\n",
      " 1134 1135 1136 1139 1140 1141 1143 1145 1147 1148 1151 1152 1153 1154\n",
      " 1156 1157 1158 1159 1160 1161 1162 1163 1165 1166 1171 1172 1173 1174\n",
      " 1175 1176 1177 1178 1179 1180 1181 1183 1184 1185 1186 1192 1193 1194\n",
      " 1195 1196 1197 1198 1202 1203 1206 1207 1208 1209 1212 1213 1215 1216\n",
      " 1217 1218 1219 1220 1221 1223 1224 1225 1226 1227 1229 1230 1231 1233\n",
      " 1234 1235 1236 1237 1238 1240 1241 1242 1243 1244 1245 1246 1247 1248\n",
      " 1252 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266\n",
      " 1267 1268 1269 1270 1271 1272 1273 1275 1277 1278 1279 1280 1281 1282\n",
      " 1283 1285 1286 1288 1289 1291 1294 1295 1297 1298 1300 1301 1303 1304\n",
      " 1305 1306 1308 1309 1312 1313 1315 1316 1317 1318 1319 1320 1321 1323\n",
      " 1324 1326 1328 1330 1331 1332 1333 1334 1335 1337 1338 1339 1340 1342\n",
      " 1343 1344 1350 1351 1355 1356 1357 1359 1360 1361 1362 1363 1364 1365\n",
      " 1366 1367 1369 1370 1372 1373 1378 1379 1382 1383 1384 1389 1390 1391\n",
      " 1392 1393 1394 1395 1396 1397 1398 1400 1401 1403 1404 1407 1408 1409\n",
      " 1411 1412 1413 1414 1415 1418 1419 1420 1421 1424 1425 1426 1429 1430\n",
      " 1432 1434 1435 1436 1437 1438 1439 1440 1441 1442 1444 1445 1446 1447\n",
      " 1448 1450 1452 1453 1455 1456 1457 1459 1460 1462 1467 1470 1471 1473\n",
      " 1474 1475 1476 1477 1479 1480 1481 1482 1483 1484 1485]\n",
      "  Test:  index=[   0    2    3    5    6    7    9   12   18   24   25   27   28   33\n",
      "   38   39   41   42   45   47   52   55   57   60   62   66   68   69\n",
      "   71   72   73   74   75   77   79   80   82   84   85   88   90   92\n",
      "   94   97  102  104  105  106  108  110  117  118  120  125  131  132\n",
      "  133  136  137  138  139  142  144  145  148  162  164  165  167  169\n",
      "  171  172  173  176  177  178  181  182  183  185  193  194  204  211\n",
      "  212  213  214  215  222  223  224  226  227  228  232  234  235  238\n",
      "  242  248  249  250  251  255  256  258  260  264  267  273  280  281\n",
      "  290  294  299  300  302  305  307  308  311  314  318  319  321  325\n",
      "  326  328  329  333  334  336  338  347  348  349  357  359  360  362\n",
      "  364  366  372  373  375  376  383  386  388  389  390  393  395  404\n",
      "  408  409  414  421  424  434  436  440  442  444  445  446  447  448\n",
      "  449  450  454  457  458  460  461  465  467  468  475  480  483  491\n",
      "  495  499  501  503  504  505  506  507  516  518  519  522  523  525\n",
      "  530  531  533  534  536  539  541  542  545  549  553  555  557  558\n",
      "  568  572  573  576  579  581  584  587  588  591  593  594  601  603\n",
      "  609  611  622  624  629  630  637  638  644  649  650  652  657  664\n",
      "  666  667  671  673  676  678  692  697  700  704  706  708  713  716\n",
      "  717  721  723  724  731  732  733  734  736  739  741  744  745  750\n",
      "  752  754  757  765  770  771  772  774  777  778  780  783  785  786\n",
      "  788  793  802  803  807  811  812  814  817  820  823  824  826  828\n",
      "  832  841  842  843  844  846  847  849  850  852  857  858  864  865\n",
      "  867  874  875  881  884  885  887  890  892  894  898  905  908  909\n",
      "  912  914  918  922  923  926  931  932  934  936  944  946  947  949\n",
      "  950  958  962  967  970  974  976  978  985  986  987  989  993  999\n",
      " 1000 1005 1010 1013 1018 1022 1023 1026 1027 1031 1032 1035 1036 1043\n",
      " 1049 1052 1054 1058 1065 1067 1074 1083 1088 1090 1093 1094 1096 1098\n",
      " 1101 1103 1107 1111 1112 1116 1117 1118 1121 1124 1125 1128 1132 1133\n",
      " 1137 1138 1142 1144 1146 1149 1150 1155 1164 1167 1168 1169 1170 1182\n",
      " 1187 1188 1189 1190 1191 1199 1200 1201 1204 1205 1210 1211 1214 1222\n",
      " 1228 1232 1239 1249 1250 1251 1253 1274 1276 1284 1287 1290 1292 1293\n",
      " 1296 1299 1302 1307 1310 1311 1314 1322 1325 1327 1329 1336 1341 1345\n",
      " 1346 1347 1348 1349 1352 1353 1354 1358 1368 1371 1374 1375 1376 1377\n",
      " 1380 1381 1385 1386 1387 1388 1399 1402 1405 1406 1410 1416 1417 1422\n",
      " 1423 1427 1428 1431 1433 1443 1449 1451 1454 1458 1461 1463 1464 1465\n",
      " 1466 1468 1469 1472 1478]\n",
      "Fold 2:\n",
      "  Train: index=[   0    2    3    5    6    7    9   10   12   15   18   23   24   25\n",
      "   27   28   29   30   31   32   33   38   39   41   42   43   44   45\n",
      "   47   48   49   51   52   54   55   56   57   58   59   60   62   63\n",
      "   65   66   67   68   69   70   71   72   73   74   75   76   77   78\n",
      "   79   80   81   82   83   84   85   86   88   90   92   94   96   97\n",
      "   99  100  101  102  104  105  106  107  108  109  110  111  113  115\n",
      "  117  118  120  123  124  125  126  128  129  131  132  133  135  136\n",
      "  137  138  139  140  141  142  144  145  147  148  155  156  158  162\n",
      "  163  164  165  167  168  169  170  171  172  173  174  175  176  177\n",
      "  178  179  181  182  183  184  185  188  192  193  194  195  196  198\n",
      "  199  203  204  208  209  210  211  212  213  214  215  218  220  221\n",
      "  222  223  224  226  227  228  231  232  233  234  235  236  237  238\n",
      "  239  240  242  243  244  247  248  249  250  251  254  255  256  258\n",
      "  259  260  261  264  265  266  267  270  271  273  274  275  277  280\n",
      "  281  282  285  286  287  289  290  291  292  294  296  297  298  299\n",
      "  300  302  303  305  306  307  308  309  310  311  312  314  316  318\n",
      "  319  321  322  323  324  325  326  327  328  329  331  332  333  334\n",
      "  336  338  339  342  344  346  347  348  349  350  351  352  353  354\n",
      "  355  357  358  359  360  361  362  363  364  365  366  367  370  371\n",
      "  372  373  374  375  376  377  380  381  382  383  386  388  389  390\n",
      "  393  394  395  398  404  405  408  409  410  411  413  414  415  416\n",
      "  419  420  421  422  423  424  425  426  427  428  429  430  432  433\n",
      "  434  435  436  438  439  440  442  444  445  446  447  448  449  450\n",
      "  451  453  454  457  458  460  461  462  464  465  467  468  471  475\n",
      "  477  478  479  480  481  482  483  485  486  490  491  493  494  495\n",
      "  497  499  500  501  503  504  505  506  507  513  514  516  518  519\n",
      "  522  523  525  526  527  528  529  530  531  532  533  534  535  536\n",
      "  538  539  541  542  543  544  545  548  549  551  552  553  554  555\n",
      "  557  558  560  561  566  567  568  570  571  572  573  575  576  578\n",
      "  579  581  582  583  584  585  587  588  589  590  591  593  594  596\n",
      "  597  598  599  601  602  603  605  607  609  610  611  613  614  615\n",
      "  617  618  619  620  621  622  624  626  628  629  630  631  634  636\n",
      "  637  638  643  644  649  650  651  652  654  657  660  661  664  665\n",
      "  666  667  668  669  670  671  672  673  674  676  677  678  679  680\n",
      "  682  691  692  693  695  697  700  701  704  705  706  707  708  710\n",
      "  711  712  713  714  715  716  717  718  720  721  722  723  724  727\n",
      "  730  731  732  733  734  736  737  739  741  743  744  745  746  750\n",
      "  752  754  755  756  757  759  762  764  765  767  770  771  772  774\n",
      "  777  778  780  781  782  783  785  786  787  788  792  793  796  798\n",
      "  799  802  803  806  807  808  809  810  811  812  813  814  816  817\n",
      "  818  819  820  823  824  826  828  829  832  834  836  838  839  841\n",
      "  842  843  844  845  846  847  849  850  852  855  857  858  859  861\n",
      "  862  864  865  867  868  869  873  874  875  881  882  884  885  886\n",
      "  887  888  889  890  892  893  894  898  899  900  901  903  904  905\n",
      "  906  907  908  909  910  912  914  915  916  917  918  921  922  923\n",
      "  924  925  926  930  931  932  933  934  936  937  938  939  940  941\n",
      "  942  943  944  945  946  947  948  949  950  952  953  958  962  963\n",
      "  964  965  966  967  968  970  973  974  976  978  979  982  983  984\n",
      "  985  986  987  988  989  990  993  994  997  999 1000 1001 1004 1005\n",
      " 1006 1009 1010 1013 1018 1022 1023 1024 1026 1027 1030 1031 1032 1033\n",
      " 1034 1035 1036 1037 1040 1041 1042 1043 1047 1048 1049 1050 1052 1053\n",
      " 1054 1055 1057 1058 1061 1063 1065 1066 1067 1068 1073 1074 1075 1078\n",
      " 1080 1083 1084 1085 1087 1088 1089 1090 1091 1093 1094 1096 1097 1098\n",
      " 1099 1101 1102 1103 1105 1106 1107 1108 1111 1112 1113 1114 1116 1117\n",
      " 1118 1119 1120 1121 1124 1125 1128 1131 1132 1133 1137 1138 1140 1141\n",
      " 1142 1144 1145 1146 1149 1150 1151 1155 1156 1157 1159 1161 1164 1165\n",
      " 1166 1167 1168 1169 1170 1173 1174 1175 1176 1177 1179 1182 1185 1187\n",
      " 1188 1189 1190 1191 1193 1195 1197 1198 1199 1200 1201 1202 1203 1204\n",
      " 1205 1206 1208 1209 1210 1211 1212 1214 1216 1218 1219 1220 1221 1222\n",
      " 1223 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1239\n",
      " 1240 1246 1249 1250 1251 1252 1253 1255 1259 1260 1261 1262 1263 1268\n",
      " 1269 1270 1272 1274 1276 1279 1280 1283 1284 1285 1287 1289 1290 1292\n",
      " 1293 1296 1298 1299 1300 1301 1302 1305 1307 1310 1311 1312 1313 1314\n",
      " 1316 1320 1321 1322 1323 1325 1327 1328 1329 1333 1334 1335 1336 1341\n",
      " 1342 1344 1345 1346 1347 1348 1349 1352 1353 1354 1355 1357 1358 1361\n",
      " 1362 1364 1366 1368 1371 1372 1374 1375 1376 1377 1378 1379 1380 1381\n",
      " 1382 1384 1385 1386 1387 1388 1389 1391 1398 1399 1401 1402 1403 1405\n",
      " 1406 1407 1408 1410 1412 1416 1417 1419 1422 1423 1425 1427 1428 1429\n",
      " 1430 1431 1432 1433 1434 1435 1438 1439 1440 1442 1443 1446 1449 1451\n",
      " 1452 1453 1454 1455 1457 1458 1460 1461 1463 1464 1465 1466 1467 1468\n",
      " 1469 1470 1472 1474 1475 1477 1478 1479 1480 1481 1485]\n",
      "  Test:  index=[   1    4    8   11   13   14   16   17   19   20   21   22   26   34\n",
      "   35   36   37   40   46   50   53   61   64   87   89   91   93   95\n",
      "   98  103  112  114  116  119  121  122  127  130  134  143  146  149\n",
      "  150  151  152  153  154  157  159  160  161  166  180  186  187  189\n",
      "  190  191  197  200  201  202  205  206  207  216  217  219  225  229\n",
      "  230  241  245  246  252  253  257  262  263  268  269  272  276  278\n",
      "  279  283  284  288  293  295  301  304  313  315  317  320  330  335\n",
      "  337  340  341  343  345  356  368  369  378  379  384  385  387  391\n",
      "  392  396  397  399  400  401  402  403  406  407  412  417  418  431\n",
      "  437  441  443  452  455  456  459  463  466  469  470  472  473  474\n",
      "  476  484  487  488  489  492  496  498  502  508  509  510  511  512\n",
      "  515  517  520  521  524  537  540  546  547  550  556  559  562  563\n",
      "  564  565  569  574  577  580  586  592  595  600  604  606  608  612\n",
      "  616  623  625  627  632  633  635  639  640  641  642  645  646  647\n",
      "  648  653  655  656  658  659  662  663  675  681  683  684  685  686\n",
      "  687  688  689  690  694  696  698  699  702  703  709  719  725  726\n",
      "  728  729  735  738  740  742  747  748  749  751  753  758  760  761\n",
      "  763  766  768  769  773  775  776  779  784  789  790  791  794  795\n",
      "  797  800  801  804  805  815  821  822  825  827  830  831  833  835\n",
      "  837  840  848  851  853  854  856  860  863  866  870  871  872  876\n",
      "  877  878  879  880  883  891  895  896  897  902  911  913  919  920\n",
      "  927  928  929  935  951  954  955  956  957  959  960  961  969  971\n",
      "  972  975  977  980  981  991  992  995  996  998 1002 1003 1007 1008\n",
      " 1011 1012 1014 1015 1016 1017 1019 1020 1021 1025 1028 1029 1038 1039\n",
      " 1044 1045 1046 1051 1056 1059 1060 1062 1064 1069 1070 1071 1072 1076\n",
      " 1077 1079 1081 1082 1086 1092 1095 1100 1104 1109 1110 1115 1122 1123\n",
      " 1126 1127 1129 1130 1134 1135 1136 1139 1143 1147 1148 1152 1153 1154\n",
      " 1158 1160 1162 1163 1171 1172 1178 1180 1181 1183 1184 1186 1192 1194\n",
      " 1196 1207 1213 1215 1217 1224 1237 1238 1241 1242 1243 1244 1245 1247\n",
      " 1248 1254 1256 1257 1258 1264 1265 1266 1267 1271 1273 1275 1277 1278\n",
      " 1281 1282 1286 1288 1291 1294 1295 1297 1303 1304 1306 1308 1309 1315\n",
      " 1317 1318 1319 1324 1326 1330 1331 1332 1337 1338 1339 1340 1343 1350\n",
      " 1351 1356 1359 1360 1363 1365 1367 1369 1370 1373 1383 1390 1392 1393\n",
      " 1394 1395 1396 1397 1400 1404 1409 1411 1413 1414 1415 1418 1420 1421\n",
      " 1424 1426 1436 1437 1441 1444 1445 1447 1448 1450 1456 1459 1462 1471\n",
      " 1473 1476 1482 1483 1484]\n"
     ]
    }
   ],
   "source": [
    "#choose 20% of the leaf terms\n",
    "import random\n",
    "random.seed(42)\n",
    "all_terms = list(is_parent.keys())\n",
    "\n",
    "k=3\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "terms = [term for term in all_terms if len(get_children(taxo, term)) == 0]\n",
    "# indexes = [i for i in range(len(terms))]\n",
    "# random.shuffle(indexes)\n",
    "# #train 80% and test 20%\n",
    "# train_index = indexes[:int(len(indexes)*0.8)]\n",
    "# test_index = indexes[int(len(indexes)*0.8):]\n",
    "kf.get_n_splits(terms)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(terms)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "    #save folds\n",
    "    with open(f\"folds/{taxo_name}/train_{i}.txt\", \"w\") as f:\n",
    "        for index in train_index:\n",
    "            f.write(terms[index] + \"\\n\")\n",
    "    with open(f\"folds/{taxo_name}/test_{i}.txt\", \"w\") as f:\n",
    "        for index in test_index:\n",
    "            f.write(terms[index] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7530c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fold\n",
    "fold = 0\n",
    "train_terms = []\n",
    "with open(f\"folds/{taxo_name}/train_{fold}.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        train_terms.append(line.strip())\n",
    "test_terms = []\n",
    "with open(f\"folds/{taxo_name}/test_{fold}.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_terms.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1287844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 496, 1555)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_terms), len(test_terms), len(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc691d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract word embeddings of terms\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4426479b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract word embeddings of terms\n",
    "import numpy as np\n",
    "\n",
    "embeddings = []\n",
    "for term in all_terms:\n",
    "    embedding = model.encode(term)\n",
    "    embeddings.append(embedding)\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fde944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 1100693.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "patterns = [\"is a\", \"is a kind of\", \"is a type of\", \"is a subtype of\", \"is a subcategory of\", \"is a subclass of\", \"is a member of\", \"is a part of\", \"is a component of\", \"is a constituent of\"]\n",
    "for i in tqdm.tqdm(range(10000)):\n",
    "    r = random.randint(0, len(train_terms)-1)\n",
    "    term = train_terms[r]\n",
    "    #get parent\n",
    "    parent = is_parent[term]\n",
    "    #positive\n",
    "    pattern = random.choice(patterns)\n",
    "    if random.random() < 0.1:        \n",
    "        #get random pattern\n",
    "        train_data.append(f\"{term} {pattern} {parent}\")\n",
    "        train_labels.append(1)\n",
    "    #negative\n",
    "    else:\n",
    "        #get any term that is not the parent\n",
    "        #get random term\n",
    "        # if random.random() < 0.5:\n",
    "        r = random.randint(0, len(train_terms)-1)\n",
    "        term2 = train_terms[r]\n",
    "        train_data.append(f\"{term} {pattern} {term2}\")\n",
    "        train_labels.append(0)\n",
    "#         else:\n",
    "#             #get most similar term that is not the parent\n",
    "#             #get embedding of term\n",
    "#             term_embedding = model.encode(term)\n",
    "#             #cosine similarity\n",
    "#             cos_sim = cosine_similarity(term_embedding.reshape(1, -1), embeddings)\n",
    "#             #now get the most similar term that is not the parent\n",
    "#             #order\n",
    "#             similarity_index = np.argsort(cos_sim[0])[::-1]\n",
    "#             #get most similar term that is not the parent\n",
    "#             for j in similarity_index:\n",
    "#                 term2 = all_terms[j]\n",
    "#                 #if term2 is not the parent\n",
    "#                 if term2 != parent and term2 != term:\n",
    "#                     break\n",
    "#             #get random pattern\n",
    "#             pattern = random.choice(patterns)\n",
    "#             train_data.append(f\"{term} {pattern} {term2}\")\n",
    "#             train_labels.append(0)\n",
    "# #get test\n",
    "test_data = []\n",
    "for i in range(len(test_terms)):\n",
    "    term = test_terms[i]\n",
    "    #get parent\n",
    "    parent = is_parent[term]\n",
    "    #get random pattern\n",
    "    pattern = random.choice(patterns)\n",
    "    test_data.append(f\"{term} {pattern} {parent}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7499a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"currant bun is a kind of eve's pudding\", 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = random.randint(0, len(train_data)-1)\n",
    "train_data[r], train_labels[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b5869b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sacchettoni is a subtype of pasta'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = random.randint(0, len(test_data)-1)\n",
    "test_data[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a7bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets train using bert\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model=BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85fb3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        #to 512\n",
    "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "        inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': inputs['labels']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13a0f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#test dataset\n",
    "for t in train_loader:\n",
    "    print(t['input_ids'].shape)\n",
    "    print(t['attention_mask'].shape)\n",
    "    print(t['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa9e5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a81c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] creamed corn is a kind of sauce [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "#decode sample 1\n",
    "sample = train_loader.dataset[0]\n",
    "print(tokenizer.decode(sample['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a631f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca8e431c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 625/625 [00:24<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Loss: 0.09122250119894743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 625/625 [00:24<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 Loss: 0.012039654421294108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 625/625 [00:24<00:00, 25.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 Loss: 0.00512825807207264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in  tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create negatives for test set\n",
    "test_labels=[1]*len(test_data)\n",
    "for i in range(len(test_data)*2):\n",
    "    r = random.randint(0, len(test_terms)-1)\n",
    "    term = test_terms[r]\n",
    "    #get parent\n",
    "    parent = is_parent[term]\n",
    "    #negative\n",
    "    #get any term that is not the parent\n",
    "    #get random term\n",
    "    # if random.random() < 0.5:\n",
    "    r = random.randint(0, len(test_terms)-1)\n",
    "    term2 = test_terms[r]\n",
    "    fake_parent = is_parent[term2]\n",
    "    #check if fake parent is not the true parent\n",
    "    while fake_parent == parent:\n",
    "        r = random.randint(0, len(test_terms)-1)\n",
    "        term2 = test_terms[r]\n",
    "        fake_parent = is_parent[term2]\n",
    "    test_data.append(f\"{term} {pattern} {fake_parent}\")\n",
    "    test_labels.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b49430b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3367003367003367\n",
      "F1 Score: 0.17837658013926364\n"
     ]
    }
   ],
   "source": [
    "#eval\n",
    "import numpy as np\n",
    "model.eval()\n",
    "test_dataset = CustomDataset(test_data, test_labels)  # Assuming all test labels are 1 (positive)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "predictions = []\n",
    "true_labels = []\n",
    "for batch in test_loader:\n",
    "    inputs_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    predictions.extend(preds)\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
